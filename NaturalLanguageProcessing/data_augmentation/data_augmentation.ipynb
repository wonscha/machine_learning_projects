{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8c210e",
   "metadata": {},
   "source": [
    "## 0-1. Import libraries\n",
    "\n",
    "Import necessary libraries and download nltk packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8b747b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wonsukcha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/wonsukcha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/wonsukcha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805330f5",
   "metadata": {},
   "source": [
    "## 0-2. Import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80426a0f",
   "metadata": {},
   "source": [
    "GoogleNews-vectors-negative300 is a pre-trainied Google News corpus word vector model.</br>\n",
    "Gensim provides word2vec class that can construct from pre-trained model. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f042b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import urllib.request\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fd6f0f",
   "metadata": {},
   "source": [
    "GoogleNews-vectors-negative300 is bigger than 3Gb. Be careful when you run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876c150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "FILENAME = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "filepath = os.path.join(os.getcwd(), FILENAME)\n",
    "urllib.request.urlretrieve(url, filepath)\n",
    "\n",
    "filepath_vec = filepath.replace('.gz', '')\n",
    "with gzip.open(filepath, 'rb') as f_in:\n",
    "            with open(filepath_vec, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ddb25",
   "metadata": {},
   "source": [
    "Create a word2vec instance using pretrained GoogleNews-vectors-negative300 vector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4319a8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_path = filepath_vec\n",
    "w2v_model = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ecdb0b",
   "metadata": {},
   "source": [
    "### 1. File selection & load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a49904",
   "metadata": {},
   "source": [
    "We are going to use a mini dataset with four tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de05e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'input_tweets_miniset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cea34b",
   "metadata": {},
   "source": [
    "First load data and store it to pandas dataframe, and drop user column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52305c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df = pd.read_csv(os.path.join(filename))\n",
    "\n",
    "# drop user column\n",
    "df = df.drop(['user'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bde9a6",
   "metadata": {},
   "source": [
    "### 2. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0d5c4",
   "metadata": {},
   "source": [
    "Create a copy of dataframe. We are going to keep intermediate steps in this copied dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea7fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of dataframe\n",
    "df_copied = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bef1f1",
   "metadata": {},
   "source": [
    "Use regular expressions of pythong methods to get rid of the unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b58ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beginning\n",
    "regex_beginning = re.compile(r'(RT\\s)?(@\\S+)')\n",
    "df_copied['preprocessed'] = df_copied['text'].str.replace(regex_beginning, '')\n",
    "\n",
    "# end\n",
    "regex_end = re.compile(r'[^. ]*â€¦')\n",
    "df_copied['preprocessed'] = df_copied['preprocessed'].str.replace(regex_end, '')\n",
    "\n",
    "# website\n",
    "regex_web = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "df_copied['preprocessed'] = df_copied['preprocessed'].str.replace(regex_web, '')\n",
    "\n",
    "# remove hashtag\n",
    "regex_hash = re.compile(r'#+[a-zA-Z0-9(_)]{1,}')\n",
    "df_copied['preprocessed'] = df_copied['preprocessed'].str.replace(regex_hash, '')\n",
    "\n",
    "# remove digits, periods, parenthes, comma, etc\n",
    "regex_unnecessary = re.compile(r\"[^a-zA-Z' ]\")\n",
    "df_copied['preprocessed'] = df_copied['preprocessed'].str.replace(regex_unnecessary, '')\n",
    "\n",
    "# lowercase and trim\n",
    "df_copied['preprocessed'] = df_copied['preprocessed'].str.lower()\n",
    "df_copied['preprocessed'] = df_copied['preprocessed'].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71281702",
   "metadata": {},
   "source": [
    "Tokenize the cleaned tweet. Since nltk tokenize \"'s\" as a token, it is better to filter out \"'s\". </br>\n",
    "With this step, we can't prevent it to be chosen as a word to be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f2354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "df_copied['preprocessed'] = df_copied['preprocessed'].apply(word_tokenize)\n",
    "df_copied['preprocessed'] = df_copied['preprocessed'].apply(lambda lst: [token for token in lst if token != \"'s\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91f1a8",
   "metadata": {},
   "source": [
    "Remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb35dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "df_copied['preprocessed'] = df_copied['preprocessed'] \\\n",
    "    .apply(lambda lst: [word for word in lst if not word in stop_words_nltk])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1600aa",
   "metadata": {},
   "source": [
    "Let's check copied dataframe. We can see preprocessed data in \"preprocessed\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "741ddbff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @cutedejun: sm not letting xiaojun go for h...</td>\n",
       "      <td>[sm, letting, xiaojun, go, graduation, brother...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Marcheline3Di For me, it was spent recovering...</td>\n",
       "      <td>[spent, recovering, allnighter, spent, music, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @OntarioHealthC: 954 ongoing #COVID19 outbr...</td>\n",
       "      <td>[ongoing, outbreaks, ontario, hospitals, retir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>RT @MicahPollak: Well, #COVID19 is once again ...</td>\n",
       "      <td>[well, leading, cause, death, based, average, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0  positive  RT @cutedejun: sm not letting xiaojun go for h...   \n",
       "1  negative  @Marcheline3Di For me, it was spent recovering...   \n",
       "2  positive  RT @OntarioHealthC: 954 ongoing #COVID19 outbr...   \n",
       "3  negative  RT @MicahPollak: Well, #COVID19 is once again ...   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  [sm, letting, xiaojun, go, graduation, brother...  \n",
       "1  [spent, recovering, allnighter, spent, music, ...  \n",
       "2  [ongoing, outbreaks, ontario, hospitals, retir...  \n",
       "3  [well, leading, cause, death, based, average, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ae389",
   "metadata": {},
   "source": [
    "### 3. Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f46953",
   "metadata": {},
   "source": [
    "Hereby we will choose two words randomly from each preprocessed tweet. </br>\n",
    "And then we will get synonyms of select words using word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b8bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(lst):\n",
    "    # get two random words\n",
    "    word1 = lst[random.randint(0, len(lst)-1)]\n",
    "    word2 = lst[random.randint(0, len(lst)-1)]\n",
    "    while word1 == word2:\n",
    "        word2 = lst[random.randint(0, len(lst)-1)]\n",
    "\n",
    "    # get synonyms. Error handling required for the case word not in present\n",
    "    try:\n",
    "        synonym1 = w2v_model.most_similar(word1)[0][0]\n",
    "    except:\n",
    "        synonym1 = None\n",
    "        \n",
    "    try:\n",
    "        synonym2 = w2v_model.most_similar(word2)[0][0]\n",
    "    except:\n",
    "        synonym2 = None\n",
    "        \n",
    "    return [(word1, synonym1), (word2, synonym2)]\n",
    "\n",
    "df_copied['synonyms'] = df_copied['preprocessed'].apply(get_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcd90e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @cutedejun: sm not letting xiaojun go for h...</td>\n",
       "      <td>[sm, letting, xiaojun, go, graduation, brother...</td>\n",
       "      <td>[(brother, younger_brother), (last, earlier)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Marcheline3Di For me, it was spent recovering...</td>\n",
       "      <td>[spent, recovering, allnighter, spent, music, ...</td>\n",
       "      <td>[(projects, project), (related, relating)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @OntarioHealthC: 954 ongoing #COVID19 outbr...</td>\n",
       "      <td>[ongoing, outbreaks, ontario, hospitals, retir...</td>\n",
       "      <td>[(high, low), (ontario, alberta)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>RT @MicahPollak: Well, #COVID19 is once again ...</td>\n",
       "      <td>[well, leading, cause, death, based, average, ...</td>\n",
       "      <td>[(deaths, fatalities), (daily, weekly)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0  positive  RT @cutedejun: sm not letting xiaojun go for h...   \n",
       "1  negative  @Marcheline3Di For me, it was spent recovering...   \n",
       "2  positive  RT @OntarioHealthC: 954 ongoing #COVID19 outbr...   \n",
       "3  negative  RT @MicahPollak: Well, #COVID19 is once again ...   \n",
       "\n",
       "                                        preprocessed  \\\n",
       "0  [sm, letting, xiaojun, go, graduation, brother...   \n",
       "1  [spent, recovering, allnighter, spent, music, ...   \n",
       "2  [ongoing, outbreaks, ontario, hospitals, retir...   \n",
       "3  [well, leading, cause, death, based, average, ...   \n",
       "\n",
       "                                        synonyms  \n",
       "0  [(brother, younger_brother), (last, earlier)]  \n",
       "1     [(projects, project), (related, relating)]  \n",
       "2              [(high, low), (ontario, alberta)]  \n",
       "3        [(deaths, fatalities), (daily, weekly)]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2c861",
   "metadata": {},
   "source": [
    "Now that we've got words to replace and synonyms of those, let's create a new dataset with texts with words being replaced with synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b77959fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_augmented = []\n",
    "for idx, synonyms in enumerate(df_copied['synonyms']):\n",
    "    text = df['text'][idx]\n",
    "    for synonym in synonyms:\n",
    "        if synonym[1] != None:\n",
    "            text = text.replace(synonym[0], synonym[1])\n",
    "    text_augmented.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5226d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_augment = {\n",
    "    'sentiment': df['sentiment'],\n",
    "    'text': pd.Series(text_augmented)\n",
    "}\n",
    "df_augmented = pd.DataFrame(dict_augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65b072",
   "metadata": {},
   "source": [
    "By concatenating two dataframes, we could get a new dataframe doubled in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4006b26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @cutedejun: sm not letting xiaojun go for h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Marcheline3Di For me, it was spent recovering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @OntarioHealthC: 954 ongoing #COVID19 outbr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>RT @MicahPollak: Well, #COVID19 is once again ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @cutedejun: sm not letting xiaojun go for h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Marcheline3Di For me, it was spent recovering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @OntarioHealthC: 954 ongoing #COVID19 outbr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>negative</td>\n",
       "      <td>RT @MicahPollak: Well, #COVID19 is once again ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0  positive  RT @cutedejun: sm not letting xiaojun go for h...\n",
       "1  negative  @Marcheline3Di For me, it was spent recovering...\n",
       "2  positive  RT @OntarioHealthC: 954 ongoing #COVID19 outbr...\n",
       "3  negative  RT @MicahPollak: Well, #COVID19 is once again ...\n",
       "4  positive  RT @cutedejun: sm not letting xiaojun go for h...\n",
       "5  negative  @Marcheline3Di For me, it was spent recovering...\n",
       "6  positive  RT @OntarioHealthC: 954 ongoing #COVID19 outbr...\n",
       "7  negative  RT @MicahPollak: Well, #COVID19 is once again ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_doubled = pd.concat([df, df_augmented], ignore_index=True)\n",
    "df_doubled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1600c",
   "metadata": {},
   "source": [
    "Export the new dataset as txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "617ff055",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doubled.to_csv('output_data.txt', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
